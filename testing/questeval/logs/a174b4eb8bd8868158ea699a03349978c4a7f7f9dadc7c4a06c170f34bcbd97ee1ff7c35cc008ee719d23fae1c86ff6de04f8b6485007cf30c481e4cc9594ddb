{
  "type": "hyp",
  "text": "Luciferase assays revealed a  \u2047 20-fold increased transcriptional activity of the 1025 bp sequence as compared to the empty promoter region indicating that we had identified an active A3G promoter sequence (Figure 3B).\n",
  "self": {
    "NER": {
      "answers": [
        "20-fold",
        "1025",
        "A3",
        "3B"
      ],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": [
          "How much did luciferase assays show increased transcriptional activity of the",
          "What bp sequence was found to have an increased transcriptional activity?",
          "What promoter was identified in the luciferase assays?",
          "What is the figure that shows that the 1025 bp sequence is active?"
        ]
      }
    },
    "NOUN": {
      "answers": [
        "a  \u2047 20-fold increased transcriptional activity",
        "the 1025 bp sequence",
        "the empty promoter region",
        "we",
        "an active A3G promoter sequence",
        "Figure 3B"
      ],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": [
          "What did luciferase assays show?",
          "What sequence was found to have a 20-fold increase in transcriptional activity?",
          "What region of the 1025 bp sequence did luciferase assay",
          "What did the luciferase assays show?",
          "What did the luciferase assays show?",
          "What is the result of the luciferase assays?"
        ]
      }
    }
  },
  "asked": {
    "How many deletions led to the 502 225 and 180 bp fragments?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0013077855110168457,
        "ground_truth": {
          "5": {
            "bertscore": 0.6699836254119873,
            "f1": 0
          }
        }
      }
    },
    "What is the number of bp fragments that were deleted?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0003559589385986328,
        "ground_truth": {
          "502 225": {
            "bertscore": 0.6362929344177246,
            "f1": 0
          }
        }
      }
    },
    "What was the number of bp fragments that were deleted?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0003631114959716797,
        "ground_truth": {
          "180": {
            "bertscore": 0.6296430826187134,
            "f1": 0
          }
        }
      }
    },
    "What is the corresponding figure for the mutagenesis of the chromosomes": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.011688768863677979,
        "ground_truth": {
          "3B": {
            "bertscore": 0.6839531064033508,
            "f1": 0
          }
        }
      }
    },
    "What was not significantly altered by the 5 deletions leading to the 502 225 and 180": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.01020050048828125,
        "ground_truth": {
          "This transcription rate": {
            "bertscore": 0.6581500768661499,
            "f1": 0
          }
        }
      }
    },
    "What did the chromosomes not significantly alter?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0007924437522888184,
        "ground_truth": {
          "the 5 deletions": {
            "bertscore": 0.6774722933769226,
            "f1": 0
          }
        }
      }
    },
    "What two deletions did the chromosomes not significantly alter?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0003705024719238281,
        "ground_truth": {
          "the 502 225 and 180 bp fragments": {
            "bertscore": 0.6220715045928955,
            "f1": 0
          }
        }
      }
    },
    "What is the result of the 5 deletions?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.00180131196975708,
        "ground_truth": {
          "Figure 3B": {
            "bertscore": 0.6591162085533142,
            "f1": 0
          }
        }
      }
    }
  }
}