{
  "type": "hyp",
  "text": "Western blot analysis demonstrated no change in PML/RARalpha or PML expression.\n",
  "self": {
    "NER": {
      "answers": [
        "PML/",
        "RARalpha",
        "PML"
      ],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": [
          "What protein did Western Blot analysis show no change in?",
          "What protein did Western blot analysis show no change in?",
          "What protein did Western blot analysis show no change in?"
        ]
      }
    },
    "NOUN": {
      "answers": [
        "Western blot analysis",
        "no change",
        "PML",
        "RARalpha",
        "PML expression"
      ],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": [
          "What showed no change in PML/RARalpha expression?",
          "What did Western blot analysis show about PML/RARalpha?",
          "What protein did Western blot analysis show no change in?",
          "What protein did Western blot analysis show no change in?",
          "What did Western blot analysis show no change in?"
        ]
      }
    }
  },
  "asked": {
    "What mRNA did the rat not express after treatment with ATRA or quercet": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.05150324106216431,
        "ground_truth": {
          "PML": {
            "bertscore": 0.6862248778343201,
            "f1": 0
          }
        }
      }
    },
    "How many cell lines were tested for the presence of ATRA?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0002955794334411621,
        "ground_truth": {
          "three": {
            "bertscore": 0.7179227471351624,
            "f1": 0
          }
        }
      }
    },
    "What was used to treat the swine flu?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0014702081680297852,
        "ground_truth": {
          "ATRA": {
            "bertscore": 0.6899303793907166,
            "f1": 0
          }
        }
      }
    },
    "What did the treatment with ATRA or quercetin not change?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.03959000110626221,
        "ground_truth": {
          "The expression": {
            "bertscore": 0.6652610301971436,
            "f1": 0
          }
        }
      }
    },
    "What did the expression of PML not change after treatment with ATRA or quercetin": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.12496405839920044,
        "ground_truth": {
          "mRNA": {
            "bertscore": 0.6875342726707458,
            "f1": 0
          }
        }
      }
    },
    "What was the effect of ATRA on the expression of PML mRNA?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.012149512767791748,
        "ground_truth": {
          "no change": {
            "bertscore": 0.7178478837013245,
            "f1": 0
          }
        }
      }
    },
    "How many cells were treated with ATRA or quercetin?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.00016963481903076172,
        "ground_truth": {
          "all the three cell lines": {
            "bertscore": 0.6818884611129761,
            "f1": 0
          }
        }
      }
    },
    "What did the expression of PML mRNA not change after?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "Western blot analysis",
        "answerability": 0.7582963705062866,
        "ground_truth": {
          "treatment": {
            "bertscore": 0.7892411947250366,
            "f1": 0
          }
        }
      }
    },
    "What drug did the mRNA expression of PML not change after treatment?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.06130087375640869,
        "ground_truth": {
          "quercetin": {
            "bertscore": 0.6064949035644531,
            "f1": 0
          }
        }
      }
    }
  }
}