{
  "type": "hyp",
  "text": "Mutations in the relevant NF-kappa B and AP-1 binding sites eliminated these responses.\n",
  "self": {
    "NER": {
      "answers": [],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": []
      }
    },
    "NOUN": {
      "answers": [
        "Mutations",
        "the relevant NF-kappa B",
        "AP-1 binding sites",
        "these responses"
      ],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": [
          "What eliminated the responses of NF-kappa B and AP-1?",
          "What two binding sites eliminated the NF-kappa responses?",
          "What NF-kappa B mutation eliminated the NF-kappa B response",
          "Mutations in the relevant NF-kappa B and AP-1 binding sites eliminated"
        ]
      }
    }
  },
  "asked": {
    "What eliminated the responses of NF-kappa B and AP-1?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "Mutations",
        "answerability": 0.9998186826705933,
        "ground_truth": {
          "Mutations": {
            "bertscore": 1.0000003576278687,
            "f1": 1.0
          }
        }
      }
    },
    "What two binding sites eliminated the NF-kappa responses?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "NF-kappa B and AP-1",
        "answerability": 0.6243658065795898,
        "ground_truth": {
          "the relevant NF-kappa B": {
            "bertscore": 0.8174264430999756,
            "f1": 0.5714285714285715
          }
        }
      }
    },
    "What NF-kappa B mutation eliminated the NF-kappa B response": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "Mutations in the relevant NF-kappa B and AP-1 binding sites",
        "answerability": 0.9678264260292053,
        "ground_truth": {
          "AP-1 binding sites": {
            "bertscore": 0.8406365513801575,
            "f1": 0.5
          }
        }
      }
    },
    "Mutations in the relevant NF-kappa B and AP-1 binding sites eliminated": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.26360946893692017,
        "ground_truth": {
          "these responses": {
            "bertscore": 0.6748326420783997,
            "f1": 0
          }
        }
      }
    }
  }
}