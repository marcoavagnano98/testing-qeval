{
  "type": "hyp",
  "text": "Both rpoE and rpoN deletions resulted in decreased SseL-2HA levels with a more pronounced effect in the rpoE deletion.\n",
  "self": {
    "NER": {
      "answers": [],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": []
      }
    },
    "NOUN": {
      "answers": [
        "Both rpoE",
        "rpoN deletions",
        "decreased SseL-2HA levels",
        "a more pronounced effect",
        "the rpoE deletion"
      ],
      "QG_hash=ThomasNLG/t5-qg_squad1-en": {
        "questions": [
          "What two deletions resulted in decreased SseL-2HA levels?",
          "What caused SseL-2HA levels to decrease?",
          "What did rpoE and rpoN results in?",
          "What effect did rpoE have on SseL-2HA levels?",
          "What was the most significant effect of the rpoN deletion?"
        ]
      }
    }
  },
  "asked": {
    "What was the lowest level of SseB?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.005264699459075928,
        "ground_truth": {
          "SifA-2HA": {
            "bertscore": 0.6338493824005127,
            "f1": 0
          }
        }
      }
    },
    "What gene did not have an effect on SseL-2HA levels?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.023333072662353516,
        "ground_truth": {
          "DeltarpoE": {
            "bertscore": 0.6498667001724243,
            "f1": 0
          }
        }
      }
    },
    "What did deletion of rpoE not have an effect on?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0007052421569824219,
        "ground_truth": {
          "SseL-2HA": {
            "bertscore": 0.6386284232139587,
            "f1": 0
          }
        }
      }
    },
    "What did the results for SseB agree with?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0008445978164672852,
        "ground_truth": {
          "the results": {
            "bertscore": 0.6694769263267517,
            "f1": 0
          }
        }
      }
    },
    "What gene did the results for DeltarpoE agree with?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.05443918704986572,
        "ground_truth": {
          "SseB": {
            "bertscore": 0.6855400800704956,
            "f1": 0
          }
        }
      }
    },
    "What was the effect of deletion of rpoE on SifA-2HA levels": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "decreased SseL-2HA levels",
        "answerability": 0.8008043169975281,
        "ground_truth": {
          "a decrease": {
            "bertscore": 0.6304653286933899,
            "f1": 0
          }
        }
      }
    },
    "What did DeltarpoE have a decrease in compared to wild type?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.05723106861114502,
        "ground_truth": {
          "SifA-2HA levels": {
            "bertscore": 0.6102402806282043,
            "f1": 0
          }
        }
      }
    },
    "What type of rpoE did not have an effect on SseL-2": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.05770796537399292,
        "ground_truth": {
          "wild type": {
            "bertscore": 0.7223243117332458,
            "f1": 0
          }
        }
      }
    },
    "What did not have an effect on SseL-2HA levels?": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.004822373390197754,
        "ground_truth": {
          "deletion": {
            "bertscore": 0.7463587522506714,
            "f1": 0
          }
        }
      }
    },
    "What did deletion of rpoE not have on SseL-2HA levels": {
      "QA_hash=ThomasNLG/t5-qa_squad2neg-en": {
        "answer": "unanswerable",
        "answerability": 0.0626603364944458,
        "ground_truth": {
          "an effect": {
            "bertscore": 0.7017390131950378,
            "f1": 0
          }
        }
      }
    }
  }
}